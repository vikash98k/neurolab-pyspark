{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.4) Consider a PySpark DataFrame named \"transactions\" with columns \"transaction_id\", \"user_id\", and \"amount\". Write a code to calculate the average transaction amount for each user and display the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/03 19:24:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/07/03 19:24:41 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/07/03 19:24:41 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+\n",
      "|transaction_id|user_id|amount|\n",
      "+--------------+-------+------+\n",
      "|            T1|     U1| 100.0|\n",
      "|            T2|     U2| 200.0|\n",
      "|            T3|     U1| 150.0|\n",
      "|            T4|     U3| 300.0|\n",
      "|            T5|     U2| 250.0|\n",
      "|            T6|     U4| 180.0|\n",
      "|            T7|     U1| 120.0|\n",
      "|            T8|     U3| 220.0|\n",
      "|            T9|     U2| 350.0|\n",
      "|           T10|     U4| 280.0|\n",
      "|           T11|     U1| 190.0|\n",
      "|           T12|     U2| 210.0|\n",
      "|           T13|     U3| 320.0|\n",
      "|           T14|     U4| 270.0|\n",
      "|           T15|     U1| 130.0|\n",
      "+--------------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Create Dummy DataFrame\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), nullable=False),\n",
    "    StructField(\"user_id\", StringType(), nullable=False),\n",
    "    StructField(\"amount\", DoubleType(), nullable=False)\n",
    "])\n",
    "\n",
    "# Create a list of dummy data\n",
    "dummy_data = [\n",
    "    (\"T1\", \"U1\", 100.0),\n",
    "    (\"T2\", \"U2\", 200.0),\n",
    "    (\"T3\", \"U1\", 150.0),\n",
    "    (\"T4\", \"U3\", 300.0),\n",
    "    (\"T5\", \"U2\", 250.0),\n",
    "    (\"T6\", \"U4\", 180.0),\n",
    "    (\"T7\", \"U1\", 120.0),\n",
    "    (\"T8\", \"U3\", 220.0),\n",
    "    (\"T9\", \"U2\", 350.0),\n",
    "    (\"T10\", \"U4\", 280.0),\n",
    "    (\"T11\", \"U1\", 190.0),\n",
    "    (\"T12\", \"U2\", 210.0),\n",
    "    (\"T13\", \"U3\", 320.0),\n",
    "    (\"T14\", \"U4\", 270.0),\n",
    "    (\"T15\", \"U1\", 130.0)\n",
    "]\n",
    "\n",
    "# Create the DataFrame\n",
    "transactions = spark.createDataFrame(dummy_data, schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "transactions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                        (1 + 63) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|user_id|    average_amount|\n",
      "+-------+------------------+\n",
      "|     U1|             138.0|\n",
      "|     U2|             252.5|\n",
      "|     U3|             280.0|\n",
      "|     U4|243.33333333333334|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 59034)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pyspark/accumulators.py\", line 262, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pyspark/accumulators.py\", line 239, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Calculate Average Transaction Amount\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Assuming \"transactions\" is the PySpark DataFrame with columns \"transaction_id\", \"user_id\", and \"amount\"\n",
    "\n",
    "# Calculate the average transaction amount for each user\n",
    "average_amount_df = transactions.groupBy(\"user_id\").agg(avg(\"amount\").alias(\"average_amount\"))\n",
    "\n",
    "# Show the result\n",
    "average_amount_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
